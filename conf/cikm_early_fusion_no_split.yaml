

defaults:
  - _self_


exp_name: cikm_early_fusion_no_split
logger_name: ${exp_name}

trainer:
    # _target_: pytorch_lightning.Trainer
    max_epochs: 32
    accelerator: gpu
    devices: 1
    auto_select_gpus: false
    enable_progress_bar: True
    gradient_clip_val: 0.5
    detect_anomaly: True
    log_every_n_steps: 50
    limit_val_batches: 512
    accumulate_grad_batches: 4
    # logger:
    #     _target_: pytorch_lightning.loggers.TensorBoardLogger
    #     save_dir: lightning_logs
    #     name: ${exp_name}
    # callbacks:
    #     - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    #       filename: '{epoch}'
    #       every_n_epochs: 1
    #       save_top_k: -1
    #       monitor: recall_top_k
    #       mode: max

data_module:
  _target_: ptls.frames.PtlsDataModule
  train_data:
    _target_: early_fusion_matching.EarlyFusionM3SupervisedIterableDataset
    splitter:
      _target_: ptls.frames.coles.split_strategy.NoSplit
    col_time: time
    cols_classes:
      - label
    mod_names:
     - data1
     - data2
    data:
        _target_: parquet_shuffle_dataset.ShuffleParquetDataset
        shuffle_files: True
        shuffle_one_file: True
        i_filters:
         - _target_: ptls.data_load.iterable_processing.feature_filter.FeatureFilter
           keep_feature_names:
            - label
           drop_feature_names: 
            - data1_uid
            - data2_uid
         - _target_: matching_bank_x_rmb.AmtProc
           amt_cols: 
         # - _target_: ptls.data_load.iterable_processing.seq_len_filter.SeqLenFilter
         #   min_seq_len: 120
         #   sequence_col: rmb_url
         - _target_: ptls.data_load.iterable_processing.to_torch_tensor.ToTorch
        data_files:
            _target_: ptls.data_load.datasets.ParquetFiles
            file_path: 
                - "/home/jovyan/shestov/src/matching/cikm_100k_mid_fusion_train_shuffle/"
  valid_data:
    _target_: early_fusion_matching.EarlyFusionM3SupervisedIterableDataset    
    splitter:
      _target_: ptls.frames.coles.split_strategy.NoSplit
    col_time: time
    cols_classes:
      - label
    mod_names:
     - data1
     - data2
    data:
        _target_: ptls.data_load.datasets.ParquetDataset
        i_filters:
         - _target_: ptls.data_load.iterable_processing.feature_filter.FeatureFilter
           keep_feature_names:
            - label
           drop_feature_names: 
            - data1_uid
            - data2_uid
         - _target_: matching_bank_x_rmb.AmtProc
           amt_cols: 
         # - _target_: ptls.data_load.iterable_processing.seq_len_filter.SeqLenFilter
         #   min_seq_len: 120
         #   sequence_col: rmb_url
         - _target_: ptls.data_load.iterable_processing.to_torch_tensor.ToTorch
        data_files:
            _target_: ptls.data_load.datasets.ParquetFiles
            file_path: 
                - "/home/jovyan/shestov/src/matching/cikm_100k_mid_fusion_val_shuffle"
  train_batch_size: 64
  train_num_workers: 8
  valid_batch_size: 64
  valid_num_workers: 8

pl_module:
  _target_: early_fusion_matching.EarlyFusionMatchingModule
  validation_metric:
    _target_: ptls.frames.coles.metric.BatchRecallTopK
    K: 1
    metric: cosine
  multimodal_seq_encoder:
    _target_: early_fusion_matching.M3SortTimeSeqEncoderContainerSameMod
    trx_encoders: 
          data1:
              _target_: ptls.nn.TrxEncoder
              embeddings_noise: 0.003
              embeddings: 
                  url0: 
                      in: 100000
                      inner: 100
                      out: 16
                  url1: 
                      in: 100000
                      inner: 100
                      out: 16
                  url2: 
                      in: 100000
                      inner: 100
                      out: 48
                  url3: 
                      in: 100000
                      inner: 100
                      out: 48
              numeric_values: 
          data2: 'data1'
    seq_encoder_cls: 
        _target_: hydra.utils.get_class
        path: ptls.nn.RnnEncoder
    input_size: 128
    col_time: "time"
    type: "gru"
    hidden_size: 128
    is_reduce_sequence: True
  loss:
    # _target_: matching-bank_x_rmb.M3SoftmaxLoss
    _target_: torch.nn.BCELoss
  optimizer_partial:
    _partial_: true
    _target_: torch.optim.AdamW
    lr: 0.001
    weight_decay: 1e-4
  lr_scheduler_partial:
    _partial_: true
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 1
    gamma: 0.9